//! # LLM API Dispatcher
//!
//! ç»Ÿä¸€çš„LLM APIè°ƒåº¦å™¨ï¼Œæ”¯æŒå¤šä¸ªä¾›åº”å•†çš„æ™ºèƒ½è·¯ç”±å’Œè´Ÿè½½å‡è¡¡
//! æ”¯æŒOllamaã€é˜¿é‡Œäº‘ã€OpenAIç­‰å¤šç§LLMä¾›åº”å•†

use std::collections::HashMap;
use serde::{Deserialize, Serialize};
use tokio::sync::RwLock;
use std::sync::Arc;
use async_trait::async_trait;
use anyhow::Result;
use std::fmt;

use crate::llm_api::utils::{
    client::ClientError,
    msg_structure::Message,
    chat_traits::{ChatRequestTrait, ChatResponseTrait},
    client_pool::{ClientPool, DynamicAliClient},
};
use crate::llm_api::ali::client::{AliClient, AliChatRequest};
use crate::llm_api::ollama::client::{OllamaClient, OllamaChatRequest};
use crate::dao::{init_sqlite_pool, init_db, SQLITE_POOL};
use crate::dao::cache::init_global_cache;
use crate::dao::provider_key_pool::preload::preload_provider_key_pools_to_cache;

// å®šä¹‰ä¾›åº”å•†æšä¸¾
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq, Hash)]
pub enum Provider {
    Ollama,
    Ali,
    OpenAI,
    Claude,
    Gemini,
}

// å®šä¹‰è¯·æ±‚å‚æ•°
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DispatchRequest {
    pub provider: Provider,
    pub model: String,
    pub messages: Vec<Message>,
    pub stream: Option<bool>,               // æ˜¯å¦æµå¼ï¼Œé»˜è®¤false
    pub temperature: Option<f32>,           // æ§åˆ¶éšæœºæ€§ï¼Œ0.0-2.0
    pub max_tokens: Option<u32>,           // æœ€å¤§ç”Ÿæˆtokenæ•°
    pub top_p: Option<f32>,                // nucleus samplingå‚æ•°
    pub frequency_penalty: Option<f32>,     // é¢‘ç‡æƒ©ç½š
    pub presence_penalty: Option<f32>,      // å­˜åœ¨æƒ©ç½š
    pub stop: Option<Vec<String>>,         // åœæ­¢è¯
    pub timeout_ms: Option<u64>,           // è¯·æ±‚è¶…æ—¶æ—¶é—´(æ¯«ç§’)
    pub retry_count: Option<u32>,          // é‡è¯•æ¬¡æ•°
    pub context_window: Option<u32>,       // ä¸Šä¸‹æ–‡çª—å£å¤§å°
}

// å®šä¹‰å“åº”ç»“æ„
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DispatchResponse {
    pub content: String,
    pub provider: Provider,
    pub model: String,
    pub usage: Option<TokenUsage>,
    pub finish_reason: Option<String>,
    pub request_id: Option<String>,
    pub created_at: String,
    pub total_duration: Option<u64>,
}

// Tokenä½¿ç”¨ç»Ÿè®¡
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TokenUsage {
    pub prompt_tokens: u32,
    pub completion_tokens: u32,
    pub total_tokens: u32,
}

// å®šä¹‰å®¢æˆ·ç«¯é€‚é…å™¨trait
#[async_trait]
pub trait LLMClientAdapter: Send + Sync {
    async fn generate(&self, request: &DispatchRequest) -> Result<DispatchResponse, LLMError>;
    async fn generate_stream(&self, request: &DispatchRequest) -> Result<tokio::sync::mpsc::Receiver<Result<String, LLMError>>, LLMError>;
    fn supported_models(&self) -> Vec<String>;
    fn provider_name(&self) -> Provider;
}

// é”™è¯¯å®šä¹‰
#[derive(Debug)]
pub enum LLMError {
    UnsupportedProvider(Provider),
    ModelNotAvailable(String),
    Timeout,
    RateLimit,
    Network(String),
    ApiError(String),
    InvalidParameters(String),
    ClientError(ClientError),
    AnyhowError(anyhow::Error),
}

impl fmt::Display for LLMError {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            LLMError::UnsupportedProvider(provider) => write!(f, "Provider not supported: {:?}", provider),
            LLMError::ModelNotAvailable(model) => write!(f, "Model not available: {}", model),
            LLMError::Timeout => write!(f, "Request timeout"),
            LLMError::RateLimit => write!(f, "Rate limited"),
            LLMError::Network(msg) => write!(f, "Network error: {}", msg),
            LLMError::ApiError(msg) => write!(f, "API error: {}", msg),
            LLMError::InvalidParameters(msg) => write!(f, "Invalid parameters: {}", msg),
            LLMError::ClientError(e) => write!(f, "Client error: {}", e),
            LLMError::AnyhowError(e) => write!(f, "Anyhow error: {}", e),
        }
    }
}

impl std::error::Error for LLMError {}

impl From<ClientError> for LLMError {
    fn from(err: ClientError) -> Self {
        LLMError::ClientError(err)
    }
}

impl From<anyhow::Error> for LLMError {
    fn from(err: anyhow::Error) -> Self {
        LLMError::AnyhowError(err)
    }
}

// Ollamaå®¢æˆ·ç«¯é€‚é…å™¨
pub struct OllamaAdapter {
    client: OllamaClient,
}

impl OllamaAdapter {
    pub fn new(client: OllamaClient) -> Self {
        Self { client }
    }
}

#[async_trait]
impl LLMClientAdapter for OllamaAdapter {
    async fn generate(&self, request: &DispatchRequest) -> Result<DispatchResponse, LLMError> {
        // æ„å»ºOllamaè¯·æ±‚
        let mut ollama_request = OllamaChatRequest::new(
            request.model.clone(),
            request.messages.clone(),
        );
        
        if let Some(stream) = request.stream {
            ollama_request.set_stream(stream);
        }
        
        // è®¾ç½®å‚æ•°
        if request.temperature.is_some() || request.max_tokens.is_some() || request.top_p.is_some() {
            let mut options = std::collections::HashMap::new();
            if let Some(temp) = request.temperature {
                options.insert("temperature".to_string(), serde_json::Value::Number(serde_json::Number::from_f64(temp as f64).unwrap()));
            }
            if let Some(max_tokens) = request.max_tokens {
                options.insert("num_predict".to_string(), serde_json::Value::Number(serde_json::Number::from(max_tokens)));
            }
            if let Some(top_p) = request.top_p {
                options.insert("top_p".to_string(), serde_json::Value::Number(serde_json::Number::from_f64(top_p as f64).unwrap()));
            }
            ollama_request.set_options(options);
        }

        // æ‰§è¡Œè¯·æ±‚
        let response = self.client.chat(ollama_request).await
            .map_err(|e| LLMError::ApiError(e.to_string()))?;

        // è½¬æ¢å“åº”
        let content = response.get_content().unwrap_or_default();
        
        Ok(DispatchResponse {
            content,
            provider: Provider::Ollama,
            model: response.get_model().to_string(),
            usage: Some(TokenUsage {
                prompt_tokens: response.get_prompt_eval_count().unwrap_or(0),
                completion_tokens: response.get_eval_count().unwrap_or(0),
                total_tokens: response.get_prompt_eval_count().unwrap_or(0) + response.get_eval_count().unwrap_or(0),
            }),
            finish_reason: if response.is_done() { Some("stop".to_string()) } else { None },
            request_id: None,
            created_at: response.get_created_at().to_string(),
            total_duration: response.get_total_duration(),
        })
    }

    async fn generate_stream(&self, _request: &DispatchRequest) -> Result<tokio::sync::mpsc::Receiver<Result<String, LLMError>>, LLMError> {
        // ç®€åŒ–å®ç°ï¼Œæš‚æ—¶ä¸æ”¯æŒæµå¼
        let (tx, rx) = tokio::sync::mpsc::channel(1);
        let _ = tx.send(Err(LLMError::InvalidParameters("Stream not implemented yet".to_string()))).await;
        Ok(rx)
    }

    fn supported_models(&self) -> Vec<String> {
        vec![
            "llama3.2".to_string(),
            "llama3.1:latest".to_string(),
            "llama3".to_string(),
            "qwen-turbo".to_string(),
            "qwen-plus".to_string(),
            "gemma2".to_string(),
            "mistral".to_string(),
            "codellama".to_string(),
        ]
    }

    fn provider_name(&self) -> Provider {
        Provider::Ollama
    }
}

// Aliå®¢æˆ·ç«¯é€‚é…å™¨
pub struct AliAdapter {
    client: AliClient,
}

impl AliAdapter {
    pub fn new(client: AliClient) -> Self {
        Self { client }
    }
}

// Aliå®¢æˆ·ç«¯æ± é€‚é…å™¨
pub struct AliPoolAdapter {
    pool: Arc<ClientPool<DynamicAliClient>>,
}

impl AliPoolAdapter {
    pub fn new(pool: Arc<ClientPool<DynamicAliClient>>) -> Self {
        Self { pool }
    }
}

#[async_trait]
impl LLMClientAdapter for AliPoolAdapter {
    async fn generate(&self, request: &DispatchRequest) -> Result<DispatchResponse, LLMError> {
        // æ„å»ºAliè¯·æ±‚
        let mut ali_request = AliChatRequest::new(
            request.model.clone(),
            request.messages.clone(),
        );
        
        if let Some(stream) = request.stream {
            ali_request.set_stream(stream);
        }
        
        // è®¾ç½®å‚æ•°
        if let Some(temp) = request.temperature {
            ali_request.temperature = Some(temp);
        }
        if let Some(max_tokens) = request.max_tokens {
            ali_request.max_tokens = Some(max_tokens);
        }
        if let Some(top_p) = request.top_p {
            ali_request.top_p = Some(top_p);
        }
        if let Some(stop) = &request.stop {
            ali_request.stop = Some(stop.clone());
        }

        // ä»æ± ä¸­è·å–å®¢æˆ·ç«¯å¹¶æ‰§è¡Œè¯·æ±‚
        let client_guard = self.pool.acquire().await;
        let client = client_guard.lock().await;
        
        let response = client.chat_with_auto_key(ali_request).await
            .map_err(|e| LLMError::ApiError(e.to_string()))?;

        // è½¬æ¢å“åº”
        let content = response.get_content().unwrap_or_default();
        let model = response.model.clone();
        let usage = response.usage.as_ref().map(|u| TokenUsage {
            prompt_tokens: u.prompt_tokens,
            completion_tokens: u.completion_tokens,
            total_tokens: u.total_tokens,
        });
        let finish_reason = response.choices.first().map(|c| c.finish_reason.clone());
        let request_id = response.id.clone();
        let created_at = response.get_created_at().to_string();
        
        Ok(DispatchResponse {
            content,
            provider: Provider::Ali,
            model,
            usage,
            finish_reason,
            request_id: Some(request_id),
            created_at,
            total_duration: None,
        })
    }

    async fn generate_stream(&self, _request: &DispatchRequest) -> Result<tokio::sync::mpsc::Receiver<Result<String, LLMError>>, LLMError> {
        // ç®€åŒ–å®ç°ï¼Œæš‚æ—¶ä¸æ”¯æŒæµå¼
        let (tx, rx) = tokio::sync::mpsc::channel(1);
        let _ = tx.send(Err(LLMError::InvalidParameters("Stream not implemented yet for pool".to_string()))).await;
        Ok(rx)
    }

    fn supported_models(&self) -> Vec<String> {
        vec![
            "qwen-plus".to_string(),
            "qwen-turbo".to_string(),
            "qwen-max".to_string(),
            "qwen-max-longcontext".to_string(),
            "qwen2.5-72b-instruct".to_string(),
            "qwen2.5-32b-instruct".to_string(),
            "qwen2.5-14b-instruct".to_string(),
            "qwen2.5-7b-instruct".to_string(),
        ]
    }

    fn provider_name(&self) -> Provider {
        Provider::Ali
    }
}

#[async_trait]
impl LLMClientAdapter for AliAdapter {
    async fn generate(&self, request: &DispatchRequest) -> Result<DispatchResponse, LLMError> {
        // æ„å»ºAliè¯·æ±‚
        let mut ali_request = AliChatRequest::new(
            request.model.clone(),
            request.messages.clone(),
        );
        
        if let Some(stream) = request.stream {
            ali_request.set_stream(stream);
        }
        
        // è®¾ç½®å‚æ•°
        if let Some(temp) = request.temperature {
            ali_request.temperature = Some(temp);
        }
        if let Some(max_tokens) = request.max_tokens {
            ali_request.max_tokens = Some(max_tokens);
        }
        if let Some(top_p) = request.top_p {
            ali_request.top_p = Some(top_p);
        }
        if let Some(stop) = &request.stop {
            ali_request.stop = Some(stop.clone());
        }

        // æ‰§è¡Œè¯·æ±‚
        let response = self.client.chat(ali_request).await
            .map_err(|e| LLMError::ApiError(e.to_string()))?;

        // è½¬æ¢å“åº”
        let content = response.get_content().unwrap_or_default();
        let model = response.model.clone();
        let usage = response.usage.as_ref().map(|u| TokenUsage {
            prompt_tokens: u.prompt_tokens,
            completion_tokens: u.completion_tokens,
            total_tokens: u.total_tokens,
        });
        let finish_reason = response.choices.first().map(|c| c.finish_reason.clone());
        let request_id = response.id.clone();
        let created_at = response.get_created_at().to_string();
        
        Ok(DispatchResponse {
            content,
            provider: Provider::Ali,
            model,
            usage,
            finish_reason,
            request_id: Some(request_id),
            created_at,
            total_duration: None,
        })
    }

    async fn generate_stream(&self, _request: &DispatchRequest) -> Result<tokio::sync::mpsc::Receiver<Result<String, LLMError>>, LLMError> {
        // ç®€åŒ–å®ç°ï¼Œæš‚æ—¶ä¸æ”¯æŒæµå¼
        let (tx, rx) = tokio::sync::mpsc::channel(1);
        let _ = tx.send(Err(LLMError::InvalidParameters("Stream not implemented yet".to_string()))).await;
        Ok(rx)
    }

    fn supported_models(&self) -> Vec<String> {
        vec![
            "qwen-plus".to_string(),
            "qwen-turbo".to_string(),
            "qwen-max".to_string(),
            "qwen-max-longcontext".to_string(),
            "qwen2.5-72b-instruct".to_string(),
            "qwen2.5-32b-instruct".to_string(),
            "qwen2.5-14b-instruct".to_string(),
            "qwen2.5-7b-instruct".to_string(),
        ]
    }

    fn provider_name(&self) -> Provider {
        Provider::Ali
    }
}

// Dispatcherä¸»ä½“
pub struct LLMDispatcher {
    clients: Arc<RwLock<HashMap<Provider, Box<dyn LLMClientAdapter>>>>,
    default_config: DispatchConfig,
}

#[derive(Debug, Clone)]
pub struct DispatchConfig {
    pub default_timeout_ms: u64,
    pub default_retry_count: u32,
    pub default_temperature: f32,
    pub enable_fallback: bool,
    pub fallback_providers: Vec<Provider>,
}

impl Default for DispatchConfig {
    fn default() -> Self {
        Self {
            default_timeout_ms: 30000,
            default_retry_count: 3,
            default_temperature: 0.7,
            enable_fallback: true,
            fallback_providers: vec![Provider::Ollama, Provider::Ali],
        }
    }
}

impl LLMDispatcher {
    pub fn new(config: Option<DispatchConfig>) -> Self {
        Self {
            clients: Arc::new(RwLock::new(HashMap::new())),
            default_config: config.unwrap_or_default(),
        }
    }

    /// åˆ›å»ºæ”¯æŒæ•°æ®åº“çš„dispatcherï¼Œè‡ªåŠ¨åˆå§‹åŒ–æ•°æ®åº“å’Œå®¢æˆ·ç«¯æ± 
    pub async fn new_with_database(config: Option<DispatchConfig>, db_url: &str, init_sql_path: &str) -> Result<Self, Box<dyn std::error::Error>> {
        // åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± 
        println!("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± ...");
        init_sqlite_pool(db_url).await;
        
        let pool = match SQLITE_POOL.get() {
            Some(pool) => {
                println!("ğŸ“¦ æ•°æ®åº“è¿æ¥æ± å·²å°±ç»ª");
                pool.clone()
            }
            None => {
                return Err("æ•°æ®åº“è¿æ¥æ± åˆå§‹åŒ–å¤±è´¥".into());
            }
        };

        // åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„
        println!("ğŸ—ï¸  æ­£åœ¨åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„...");
        match init_db(init_sql_path).await {
            Ok(_) => println!("âœ… æ•°æ®åº“è¡¨ç»“æ„åˆå§‹åŒ–å®Œæˆ"),
            Err(e) => {
                eprintln!("âŒ æ•°æ®åº“è¡¨ç»“æ„åˆå§‹åŒ–å¤±è´¥: {}", e);
                return Err(e.into());
            }
        }

        // åˆå§‹åŒ–ç¼“å­˜
        println!("ğŸ’¾ æ­£åœ¨åˆå§‹åŒ–å†…å­˜ç¼“å­˜...");
        match init_global_cache(&pool, 3600, 1000).await {
            Ok(_) => println!("âœ… å†…å­˜ç¼“å­˜åˆå§‹åŒ–å®Œæˆ"),
            Err(e) => {
                eprintln!("âŒ å†…å­˜ç¼“å­˜åˆå§‹åŒ–å¤±è´¥: {}", e);
                return Err(e.into());
            }
        }
        
        // é¢„åŠ è½½ API Key åˆ°å†…å­˜
        println!("ğŸ”„ æ­£åœ¨é¢„åŠ è½½ API Key åˆ°å†…å­˜...");
        preload_provider_key_pools_to_cache(&pool).await?;
        println!("âœ… API Key é¢„åŠ è½½å®Œæˆ");

        // åˆ›å»ºdispatcher
        let dispatcher = Self::new(config);
        
        Ok(dispatcher)
    }

    /// æ³¨å†ŒAliå®¢æˆ·ç«¯æ± 
    pub async fn register_ali_pool(&self, pool_size: usize) -> Result<(), Box<dyn std::error::Error>> {
        println!("ğŸŠ æ­£åœ¨åˆå§‹åŒ–é˜¿é‡Œäº‘å®¢æˆ·ç«¯æ± ...");
        
        // åˆ›å»ºå¤šä¸ªDynamicAliClientå®ä¾‹
        let mut clients = Vec::new();
        for _ in 0..pool_size {
            let client = DynamicAliClient::new()?;
            clients.push(client);
        }
        
        let pool = Arc::new(ClientPool::new(clients));
        let adapter = AliPoolAdapter::new(pool);
        
        self.register_client(Box::new(adapter)).await;
        println!("âœ… é˜¿é‡Œäº‘å®¢æˆ·ç«¯æ± åˆå§‹åŒ–å®Œæˆ (å¤§å°: {})", pool_size);
        
        Ok(())
    }

    // æ³¨å†Œå®¢æˆ·ç«¯
    pub async fn register_client(&self, client: Box<dyn LLMClientAdapter>) {
        let provider = client.provider_name();
        let mut clients = self.clients.write().await;
        clients.insert(provider, client);
    }

    // æ‰¹é‡æ³¨å†Œå®¢æˆ·ç«¯
    pub async fn register_clients(&self, clients: Vec<Box<dyn LLMClientAdapter>>) {
        for client in clients {
            self.register_client(client).await;
        }
    }

    // ä¸»è¦çš„dispatchæ–¹æ³•
    pub async fn dispatch(&self, mut request: DispatchRequest) -> Result<DispatchResponse, LLMError> {
        // åº”ç”¨é»˜è®¤é…ç½®
        self.apply_defaults(&mut request);

        // éªŒè¯è¯·æ±‚å‚æ•°
        self.validate_request(&request)?;

        // è·å–å®¢æˆ·ç«¯å¹¶æ‰§è¡Œ
        let result = self.dispatch_internal(&request).await;

        // å¦‚æœå¯ç”¨äº†fallbackä¸”è¯·æ±‚å¤±è´¥ï¼Œå°è¯•å¤‡é€‰ä¾›åº”å•†
        match result {
            Err(e) if self.default_config.enable_fallback => {
                self.try_fallback(request, e).await
            }
            other => other,
        }
    }

    // æµå¼dispatch
    pub async fn dispatch_stream(&self, mut request: DispatchRequest) -> Result<tokio::sync::mpsc::Receiver<Result<String, LLMError>>, LLMError> {
        self.apply_defaults(&mut request);
        self.validate_request(&request)?;

        let clients = self.clients.read().await;
        let client = clients.get(&request.provider)
            .ok_or_else(|| LLMError::UnsupportedProvider(request.provider.clone()))?;

        client.generate_stream(&request).await
    }

    // è·å–æ‰€æœ‰æ”¯æŒçš„æ¨¡å‹
    pub async fn list_models(&self, provider: Option<Provider>) -> HashMap<Provider, Vec<String>> {
        let clients = self.clients.read().await;
        let mut models = HashMap::new();

        if let Some(p) = provider {
            if let Some(client) = clients.get(&p) {
                models.insert(p, client.supported_models());
            }
        } else {
            for (provider, client) in clients.iter() {
                models.insert(provider.clone(), client.supported_models());
            }
        }

        models
    }

    // æ£€æŸ¥ä¾›åº”å•†æ˜¯å¦å¯ç”¨
    pub async fn is_provider_available(&self, provider: &Provider) -> bool {
        let clients = self.clients.read().await;
        clients.contains_key(provider)
    }

    // å†…éƒ¨dispatchå®ç°
    async fn dispatch_internal(&self, request: &DispatchRequest) -> Result<DispatchResponse, LLMError> {
        let clients = self.clients.read().await;
        let client = clients.get(&request.provider)
            .ok_or_else(|| LLMError::UnsupportedProvider(request.provider.clone()))?;

        // æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒ
        if !client.supported_models().contains(&request.model) {
            return Err(LLMError::ModelNotAvailable(request.model.clone()));
        }

        // æ‰§è¡Œè¯·æ±‚ï¼Œå¸¦é‡è¯•é€»è¾‘
        let retry_count = request.retry_count.unwrap_or(self.default_config.default_retry_count);
        let mut last_error = None;

        for attempt in 0..=retry_count {
            match client.generate(request).await {
                Ok(response) => return Ok(response),
                Err(e) => {
                    last_error = Some(e);
                    if attempt < retry_count {
                        // ç®€å•çš„é€€é¿ç­–ç•¥
                        tokio::time::sleep(tokio::time::Duration::from_millis(1000 * (attempt + 1) as u64)).await;
                    }
                }
            }
        }

        Err(last_error.unwrap())
    }

    // å°è¯•å¤‡é€‰ä¾›åº”å•†
    async fn try_fallback(&self, mut request: DispatchRequest, original_error: LLMError) -> Result<DispatchResponse, LLMError> {
        for fallback_provider in &self.default_config.fallback_providers {
            if *fallback_provider == request.provider {
                continue; // è·³è¿‡åŸå§‹ä¾›åº”å•†
            }

            request.provider = fallback_provider.clone();
            if let Ok(response) = self.dispatch_internal(&request).await {
                return Ok(response);
            }
        }

        // æ‰€æœ‰å¤‡é€‰éƒ½å¤±è´¥ï¼Œè¿”å›åŸå§‹é”™è¯¯
        Err(original_error)
    }

    // åº”ç”¨é»˜è®¤é…ç½®
    fn apply_defaults(&self, request: &mut DispatchRequest) {
        if request.temperature.is_none() {
            request.temperature = Some(self.default_config.default_temperature);
        }
        if request.timeout_ms.is_none() {
            request.timeout_ms = Some(self.default_config.default_timeout_ms);
        }
        if request.retry_count.is_none() {
            request.retry_count = Some(self.default_config.default_retry_count);
        }
    }

    // éªŒè¯è¯·æ±‚å‚æ•°
    fn validate_request(&self, request: &DispatchRequest) -> Result<(), LLMError> {
        if request.messages.is_empty() {
            return Err(LLMError::InvalidParameters("Messages cannot be empty".to_string()));
        }

        if request.model.is_empty() {
            return Err(LLMError::InvalidParameters("Model cannot be empty".to_string()));
        }

        if let Some(temp) = request.temperature {
            if temp < 0.0 || temp > 2.0 {
                return Err(LLMError::InvalidParameters("Temperature must be between 0.0 and 2.0".to_string()));
            }
        }

        Ok(())
    }
}

// ä¾¿æ·æ–¹æ³•
impl DispatchRequest {
    pub fn new(provider: Provider, model: String, messages: Vec<Message>) -> Self {
        Self {
            provider,
            model,
            messages,
            stream: None,
            temperature: None,
            max_tokens: None,
            top_p: None,
            frequency_penalty: None,
            presence_penalty: None,
            stop: None,
            timeout_ms: None,
            retry_count: None,
            context_window: None,
        }
    }

    pub fn with_stream(mut self, stream: bool) -> Self {
        self.stream = Some(stream);
        self
    }

    pub fn with_temperature(mut self, temperature: f32) -> Self {
        self.temperature = Some(temperature);
        self
    }

    pub fn with_max_tokens(mut self, max_tokens: u32) -> Self {
        self.max_tokens = Some(max_tokens);
        self
    }

    pub fn with_top_p(mut self, top_p: f32) -> Self {
        self.top_p = Some(top_p);
        self
    }

    pub fn with_stop(mut self, stop: Vec<String>) -> Self {
        self.stop = Some(stop);
        self
    }
}