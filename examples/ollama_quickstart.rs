//! # OllamaClient å¿«é€Ÿå¼€å§‹ç¤ºä¾‹
//!
//! è¿™æ˜¯ä¸€ä¸ªæœ€ç®€å•çš„ OllamaClient ä½¿ç”¨ç¤ºä¾‹
//! 
//! è¿è¡Œå‰è¯·ç¡®ä¿ï¼š
//! 1. Ollama æœåŠ¡æ­£åœ¨è¿è¡Œï¼š`ollama serve`
//! 2. å·²å®‰è£…æ¨¡å‹ï¼š`ollama pull llama3.2`
//!
//! è¿è¡Œæ–¹å¼ï¼š
//! ```bash
//! cargo run --example ollama_quickstart
//! ```

use anyhow::Result;
use project_rust_learn::llm_api::ollama::client::{OllamaClient, OllamaChatRequest};
use project_rust_learn::llm_api::utils::msg_structure::Message;

#[tokio::main]
async fn main() -> Result<()> {
    println!("ğŸš€ OllamaClient å¿«é€Ÿå¼€å§‹");
    println!("=====================");
    
    // 1. åˆ›å»ºå®¢æˆ·ç«¯ï¼ˆä½¿ç”¨é»˜è®¤é…ç½®ï¼‰
    let client = OllamaClient::new("http://localhost:11434".to_string())?;
    println!("âœ… å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ");
    
    // 2. æµ‹è¯•è¿æ¥å¹¶è·å–æ¨¡å‹åˆ—è¡¨
    println!("\nğŸ“‹ è·å–å¯ç”¨æ¨¡å‹...");
    let selected_model = match client.list_models().await {
        Ok(models) => {
            if models.is_empty() {
                println!("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨æ¨¡å‹");
                println!("è¯·è¿è¡Œ: ollama pull llama3.2");
                return Ok(());
            }
            println!("âœ… æ‰¾åˆ°æ¨¡å‹: {:?}", models);
            
            // ä»æ¨¡å‹åˆ—è¡¨ä¸­é€‰æ‹©ä¸€ä¸ª llama3.x æ¨¡å‹
            let llama_model = models.iter()
                .find(|model| model.to_lowercase().contains("llama"))
                .cloned();
            
            match llama_model {
                Some(model) => {
                    println!("ğŸ¯ é€‰æ‹©æ¨¡å‹: {}", model);
                    model
                }
                None => {
                    println!("âŒ æ²¡æœ‰æ‰¾åˆ° llama3.x æ¨¡å‹");
                    println!("è¯·è¿è¡Œ: ollama pull llama3.2");
                    return Ok(());
                }
            }
        }
        Err(e) => {
            println!("âŒ è¿æ¥å¤±è´¥: {}", e);
            println!("è¯·ç¡®ä¿ Ollama æ­£åœ¨è¿è¡Œ: ollama serve");
            return Ok(());
        }
    };
    
    // 3. åˆ›å»ºç®€å•å¯¹è¯
    let messages = vec![
        Message {
            role: "user".to_string(),
            content: "ä½ å¥½ï¼è¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±ã€‚".to_string(),
            thinking: None,
            images: None,
            tool_calls: None,
            tool_name: None,
        },
    ];
    
    // 4. å‘é€è¯·æ±‚
    let request = OllamaChatRequest::new(selected_model, messages);
    
    println!("\nğŸ’¬ å‘é€æ¶ˆæ¯ä¸­...");
    match client.chat(request).await {
        Ok(response) => {
            println!("âœ… æ”¶åˆ°å›å¤:");
            if let Some(message) = response.message {
                println!("ğŸ¤– {}", message.content);
            }
            
            // æ˜¾ç¤ºä¸€äº›ç»Ÿè®¡ä¿¡æ¯
            if let Some(duration) = response.total_duration {
                println!("\nğŸ“Š è€—æ—¶: {:.2} ç§’", duration as f64 / 1_000_000_000.0);
            }
            if let Some(tokens) = response.eval_count {
                println!("ğŸ“Š ç”Ÿæˆ tokens: {}", tokens);
            }
        }
        Err(e) => {
            println!("âŒ è¯·æ±‚å¤±è´¥: {}", e);
        }
    }
    
    println!("\nğŸ‰ ç¤ºä¾‹å®Œæˆï¼");
    println!("\nğŸ’¡ æ¥ä¸‹æ¥ä½ å¯ä»¥å°è¯•:");
    println!("   - ä¿®æ”¹æ¶ˆæ¯å†…å®¹");
    println!("   - å°è¯•ä¸åŒçš„æ¨¡å‹");
    println!("   - æŸ¥çœ‹å®Œæ•´ç¤ºä¾‹: cargo run --example ollama_client_demo");
    
    Ok(())
}
