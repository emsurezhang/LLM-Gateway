# LLM Dispatcher ä½¿ç”¨æŒ‡å—

LLM Dispatcher æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„LLM APIè°ƒåº¦å™¨ï¼Œæ”¯æŒå¤šä¸ªä¾›åº”å•†çš„æ™ºèƒ½è·¯ç”±å’Œè´Ÿè½½å‡è¡¡ã€‚

## ç‰¹æ€§

- ğŸ”€ **ç»Ÿä¸€æ¥å£**: ä¸€ä¸ªæ¥å£è°ƒç”¨å¤šç§LLMä¾›åº”å•†
- ğŸ”„ **è‡ªåŠ¨Fallback**: å½“ä¸»ä¾›åº”å•†å¤±è´¥æ—¶è‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡é€‰ä¾›åº”å•†  
- âš¡ **å¼‚æ­¥æ”¯æŒ**: å…¨å¼‚æ­¥è®¾è®¡ï¼Œæ”¯æŒé«˜å¹¶å‘
- ğŸ›ï¸ **å‚æ•°ç»Ÿä¸€**: ç»Ÿä¸€çš„å‚æ•°æ ¼å¼ï¼Œè‡ªåŠ¨é€‚é…ä¸åŒä¾›åº”å•†
- ğŸ”§ **çµæ´»é…ç½®**: æ”¯æŒè¶…æ—¶ã€é‡è¯•ã€æ¸©åº¦ç­‰å‚æ•°é…ç½®
- ğŸ“Š **ä½¿ç”¨ç»Ÿè®¡**: è¿”å›Tokenä½¿ç”¨é‡ç­‰ç»Ÿè®¡ä¿¡æ¯

## æ”¯æŒçš„ä¾›åº”å•†

- **Ollama**: æœ¬åœ°LLMæœåŠ¡ (llama3.2, qwen2.5, gemma2ç­‰)
- **é˜¿é‡Œäº‘**: é€šä¹‰åƒé—®ç³»åˆ— (qwen-plus, qwen-turbo, qwen-maxç­‰)
- **OpenAI**: GPTç³»åˆ— (å³å°†æ”¯æŒ)
- **Claude**: Anthropic Claude (å³å°†æ”¯æŒ)

## å¿«é€Ÿå¼€å§‹

### 1. åŸºç¡€ä½¿ç”¨

```rust
use project_rust_learn::llm_api::{
    dispatcher::{LLMDispatcher, DispatchRequest, Provider, OllamaAdapter},
    utils::msg_structure::Message,
    ollama::client::OllamaClient,
};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // åˆ›å»ºdispatcher
    let dispatcher = LLMDispatcher::new(None);

    // æ³¨å†ŒOllamaå®¢æˆ·ç«¯
    let ollama_client = OllamaClient::new("http://localhost:11434".to_string())?;
    dispatcher.register_client(Box::new(OllamaAdapter::new(ollama_client))).await;

    // å‡†å¤‡æ¶ˆæ¯
    let messages = vec![Message {
        role: "user".to_string(),
        content: "Hello, world!".to_string(),
        thinking: None,
        images: None, 
        tool_calls: None,
        tool_name: None,
    }];

    // å‘é€è¯·æ±‚
    let request = DispatchRequest::new(
        Provider::Ollama,
        "llama3.2".to_string(),
        messages,
    ).with_temperature(0.7);

    let response = dispatcher.dispatch(request).await?;
    println!("å›å¤: {}", response.content);

    Ok(())
}
```

### 2. å¤šä¾›åº”å•†é…ç½®

```rust
use std::env;

// åˆ›å»ºdispatcheré…ç½®
let config = DispatchConfig {
    default_timeout_ms: 30000,
    default_retry_count: 3,
    default_temperature: 0.7,
    enable_fallback: true,
    fallback_providers: vec![Provider::Ollama, Provider::Ali],
};

let dispatcher = LLMDispatcher::new(Some(config));

// æ³¨å†Œå¤šä¸ªå®¢æˆ·ç«¯
let ollama_client = OllamaClient::new("http://localhost:11434".to_string())?;
dispatcher.register_client(Box::new(OllamaAdapter::new(ollama_client))).await;

if let Ok(api_key) = env::var("DASHSCOPE_API_KEY") {
    let ali_client = AliClient::new(api_key)?;
    dispatcher.register_client(Box::new(AliAdapter::new(ali_client))).await;
}
```

### 3. å‚æ•°é…ç½®

```rust
let request = DispatchRequest::new(
    Provider::Ali,
    "qwen-turbo".to_string(),
    messages,
)
.with_temperature(0.8)           // åˆ›é€ æ€§æ§åˆ¶
.with_max_tokens(1000)          // æœ€å¤§è¾“å‡ºé•¿åº¦
.with_top_p(0.9)                // nucleus sampling
.with_stop(vec!["END".to_string()]); // åœæ­¢è¯

let response = dispatcher.dispatch(request).await?;
```

### 4. æµå¼å“åº” (å¼€å‘ä¸­)

```rust
// æ³¨æ„ï¼šæµå¼åŠŸèƒ½è¿˜åœ¨å¼€å‘ä¸­
let request = DispatchRequest::new(
    Provider::Ollama,
    "llama3.2".to_string(),
    messages,
).with_stream(true);

let mut stream = dispatcher.dispatch_stream(request).await?;
while let Some(chunk) = stream.recv().await {
    match chunk {
        Ok(content) => print!("{}", content),
        Err(e) => eprintln!("é”™è¯¯: {}", e),
    }
}
```

## ç¯å¢ƒè®¾ç½®

### Ollamaè®¾ç½®

```bash
# å®‰è£…Ollama
curl -fsSL https://ollama.com/install.sh | sh

# å¯åŠ¨æœåŠ¡
ollama serve

# ä¸‹è½½æ¨¡å‹
ollama pull llama3.2
ollama pull qwen2.5
```

### é˜¿é‡Œäº‘è®¾ç½®

```bash
# è®¾ç½®API Keyç¯å¢ƒå˜é‡
export DASHSCOPE_API_KEY="your-dashscope-api-key"
```

## è¿è¡Œç¤ºä¾‹

```bash
# åŸºç¡€ç¤ºä¾‹
cargo run --example simple_dispatcher_demo

# å®Œæ•´åŠŸèƒ½ç¤ºä¾‹  
cargo run --example dispatcher_demo

# è®¾ç½®é˜¿é‡Œäº‘API Keyåè¿è¡Œ
DASHSCOPE_API_KEY=your-key cargo run --example simple_dispatcher_demo
```

## APIå‚è€ƒ

### DispatchRequest å‚æ•°

| å‚æ•° | ç±»å‹ | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|------|--------|
| provider | Provider | ä¾›åº”å•†é€‰æ‹© | - |
| model | String | æ¨¡å‹åç§° | - |
| messages | Vec<Message> | å¯¹è¯æ¶ˆæ¯ | - |
| stream | Option<bool> | æ˜¯å¦æµå¼ | false |
| temperature | Option<f32> | éšæœºæ€§(0.0-2.0) | 0.7 |
| max_tokens | Option<u32> | æœ€å¤§è¾“å‡ºtoken | - |
| top_p | Option<f32> | nucleus sampling | - |
| stop | Option<Vec<String>> | åœæ­¢è¯ | - |
| timeout_ms | Option<u64> | è¶…æ—¶(æ¯«ç§’) | 30000 |
| retry_count | Option<u32> | é‡è¯•æ¬¡æ•° | 3 |

### DispatchResponse å­—æ®µ

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| content | String | AIç”Ÿæˆçš„å†…å®¹ |
| provider | Provider | å®é™…ä½¿ç”¨çš„ä¾›åº”å•† |
| model | String | å®é™…ä½¿ç”¨çš„æ¨¡å‹ |
| usage | Option<TokenUsage> | Tokenä½¿ç”¨ç»Ÿè®¡ |
| finish_reason | Option<String> | å®ŒæˆåŸå›  |
| request_id | Option<String> | è¯·æ±‚ID |
| created_at | String | åˆ›å»ºæ—¶é—´ |
| total_duration | Option<u64> | æ€»è€—æ—¶(çº³ç§’) |

## é”™è¯¯å¤„ç†

```rust
use project_rust_learn::llm_api::dispatcher::LLMError;

match dispatcher.dispatch(request).await {
    Ok(response) => {
        println!("æˆåŠŸ: {}", response.content);
    }
    Err(LLMError::UnsupportedProvider(provider)) => {
        println!("ä¸æ”¯æŒçš„ä¾›åº”å•†: {:?}", provider);
    }
    Err(LLMError::ModelNotAvailable(model)) => {
        println!("æ¨¡å‹ä¸å¯ç”¨: {}", model);
    }
    Err(LLMError::ApiError(msg)) => {
        println!("APIé”™è¯¯: {}", msg);
    }
    Err(e) => {
        println!("å…¶ä»–é”™è¯¯: {}", e);
    }
}
```

## æœ€ä½³å®è·µ

1. **ä¾›åº”å•†é€‰æ‹©**: ä¼˜å…ˆä½¿ç”¨æœ¬åœ°Ollamaåšå¼€å‘æµ‹è¯•ï¼Œç”Ÿäº§ç¯å¢ƒä½¿ç”¨äº‘æœåŠ¡
2. **Fallbacké…ç½®**: å¯ç”¨fallbackæœºåˆ¶ï¼Œæé«˜ç³»ç»Ÿå¯ç”¨æ€§
3. **å‚æ•°è°ƒä¼˜**: æ ¹æ®ä»»åŠ¡ç±»å‹è°ƒæ•´temperatureå’Œtop_på‚æ•°
4. **é”™è¯¯å¤„ç†**: å……åˆ†å¤„ç†å„ç§é”™è¯¯æƒ…å†µï¼Œæä¾›ç”¨æˆ·å‹å¥½çš„æç¤º
5. **ç›‘æ§ç»Ÿè®¡**: åˆ©ç”¨usageä¿¡æ¯ç›‘æ§APIä½¿ç”¨æƒ…å†µå’Œæˆæœ¬

## æ•…éšœæ’é™¤

### Ollamaè¿æ¥å¤±è´¥
- ç¡®ä¿OllamaæœåŠ¡æ­£åœ¨è¿è¡Œ: `ollama serve`
- æ£€æŸ¥ç«¯å£æ˜¯å¦æ­£ç¡®: é»˜è®¤11434
- ç¡®è®¤æ¨¡å‹å·²ä¸‹è½½: `ollama list`

### é˜¿é‡Œäº‘APIå¤±è´¥  
- æ£€æŸ¥API Keyæ˜¯å¦æ­£ç¡®è®¾ç½®
- ç¡®è®¤è´¦æˆ·ä½™é¢å……è¶³
- æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®

### æ¨¡å‹ä¸å¯ç”¨
- Ollama: `ollama pull <model-name>`
- é˜¿é‡Œäº‘: å‚è€ƒå®˜æ–¹æ–‡æ¡£ç¡®è®¤æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨

éœ€è¦å¸®åŠ©ï¼Ÿæäº¤Issueæˆ–æŸ¥çœ‹æ›´å¤šç¤ºä¾‹ä»£ç ã€‚
