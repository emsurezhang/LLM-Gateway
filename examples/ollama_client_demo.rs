//! # OllamaClient æ¼”ç¤ºç¨‹åº
//!
//! è¿™æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„æ¼”ç¤ºç¨‹åºï¼Œå±•ç¤ºå¦‚ä½•ä½¿ç”¨ OllamaClient
//! 
//! è¿è¡Œæ–¹å¼ï¼š
//! ```bash
//! cargo run --example ollama_client_demo
//! ```

use anyhow::Result;
use std::collections::HashMap;
use serde_json::Value;

use project_rust_learn::llm_api::ollama::client::{OllamaClient, OllamaChatRequest};
use project_rust_learn::llm_api::utils::{
    client::{ClientConfig, TimeoutConfig, RetryConfig},
    msg_structure::Message,
    tool_structure::{Tool, ToolFunction},
    chat_traits::ChatRequestTrait,
};



/// åˆå§‹åŒ–æ¨¡å‹é€‰æ‹©
async fn initialize_model(client: &OllamaClient) -> Result<String> {
    println!("ğŸ“‹ æ£€æŸ¥å¯ç”¨æ¨¡å‹...");
    match client.list_models().await {
        Ok(models) => {
            if models.is_empty() {
                println!("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨æ¨¡å‹");
                println!("è¯·è¿è¡Œ: ollama pull llama3.2");
                return Err(anyhow::anyhow!("æ²¡æœ‰å¯ç”¨æ¨¡å‹"));
            }
            println!("âœ… æ‰¾åˆ°æ¨¡å‹: {:?}", models);
            
            // ä»æ¨¡å‹åˆ—è¡¨ä¸­é€‰æ‹©ä¸€ä¸ª llama æ¨¡å‹
            let llama_model = models.iter()
                .find(|model| model.to_lowercase().contains("llama"))
                .cloned();
            
            match llama_model {
                Some(model) => {
                    println!("ğŸ¯ é€‰æ‹©æ¨¡å‹: {}", model);
                    Ok(model)
                }
                None => {
                    println!("âŒ æ²¡æœ‰æ‰¾åˆ° llama æ¨¡å‹");
                    println!("è¯·è¿è¡Œ: ollama pull llama3.2");
                    Err(anyhow::anyhow!("æ²¡æœ‰æ‰¾åˆ° llama æ¨¡å‹"))
                }
            }
        }
        Err(e) => {
            println!("âŒ æ— æ³•è¿æ¥åˆ° Ollama: {}", e);
            println!("è¯·ç¡®ä¿ Ollama æ­£åœ¨è¿è¡Œï¼šollama serve");
            Err(anyhow::anyhow!("è¿æ¥å¤±è´¥: {}", e))
        }
    }
}

/// åŸºç¡€ä½¿ç”¨ç¤ºä¾‹
async fn basic_example(model: &str) -> Result<()> {
    println!("=== åŸºç¡€ä½¿ç”¨ç¤ºä¾‹ ===");
    
    // 1. åˆ›å»ºå®¢æˆ·ç«¯
    let client = OllamaClient::new("http://localhost:11434".to_string())?;
    
    // 2. æ„å»ºå¯¹è¯
    let messages = vec![
        Message {
            role: "system".to_string(),
            content: "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹".to_string(),
            thinking: None,
            images: None,
            tool_calls: None,
            tool_name: None,
        },
        Message {
            role: "user".to_string(),
            content: "ç®€å•ä»‹ç»ä¸€ä¸‹ Rust ç¼–ç¨‹è¯­è¨€".to_string(),
            thinking: None,
            images: None,
            tool_calls: None,
            tool_name: None,
        },
    ];
    
    let request = OllamaChatRequest::new(model.to_string(), messages);
    
    // 4. å‘é€è¯·æ±‚
    println!("\nå‘é€è¯·æ±‚ä¸­...");
    match client.chat(request).await {
        Ok(response) => {
            println!("âœ… è¯·æ±‚æˆåŠŸï¼");
            println!("æ¨¡å‹: {}", response.model);
            if let Some(message) = response.message {
                println!("å›å¤: {}", message.content);
            }
            if let Some(duration) = response.total_duration {
                println!("è€—æ—¶: {:.2}ms", duration as f64 / 1_000_000.0);
            }
        }
        Err(e) => {
            println!("âŒ è¯·æ±‚å¤±è´¥: {}", e);
        }
    }
    
    Ok(())
}

/// æµå¼è¾“å‡ºç¤ºä¾‹
async fn streaming_example(model: &str) -> Result<()> {
    println!("\n=== æµå¼è¾“å‡ºç¤ºä¾‹ ===");
    
    let client = OllamaClient::new("http://localhost:11434".to_string())?;
    
    let messages = vec![
        Message {
            role: "user".to_string(),
            content: "å†™ä¸€é¦–å…³äºç¼–ç¨‹çš„çŸ­è¯—".to_string(),
            thinking: None,
            images: None,
            tool_calls: None,
            tool_name: None,
        },
    ];
    
    let request = OllamaChatRequest::new(model.to_string(), messages);
    
    println!("å¼€å§‹æµå¼è¾“å‡º:");
    print!("å›å¤: ");
    
    match client.chat_stream(request, |response| {
        if let Some(message) = &response.message {
            print!("{}", message.content);
            std::io::Write::flush(&mut std::io::stdout()).unwrap();
        }
        !response.done
    }).await {
        Ok(_) => println!("\nâœ… æµå¼è¾“å‡ºå®Œæˆï¼"),
        Err(e) => println!("\nâŒ æµå¼è¾“å‡ºå¤±è´¥: {}", e),
    }
    
    Ok(())
}

/// è‡ªå®šä¹‰é…ç½®ç¤ºä¾‹
async fn custom_config_example(model: &str) -> Result<()> {
    println!("\n=== è‡ªå®šä¹‰é…ç½®ç¤ºä¾‹ ===");
    
    // è‡ªå®šä¹‰å®¢æˆ·ç«¯é…ç½®
    let config = ClientConfig::new()
        .with_user_agent("OllamaDemo/1.0".to_string())
        .with_timeout(TimeoutConfig::new().with_request_timeout(std::time::Duration::from_secs(30)))
        .with_retry(RetryConfig::new().with_max_attempts(2).with_base_delay(std::time::Duration::from_millis(500)));
    
    let client = OllamaClient::new_with_config(
        "http://localhost:11434".to_string(),
        config
    )?;
    
    let messages = vec![
        Message {
            role: "user".to_string(),
            content: "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿè¯·ç®€çŸ­å›ç­”".to_string(),
            thinking: None,
            images: None,
            tool_calls: None,
            tool_name: None,
        },
    ];
    
    let mut request = OllamaChatRequest::new(model.to_string(), messages);
    
    // è®¾ç½®æ¨¡å‹å‚æ•°
    let mut options = HashMap::new();
    options.insert("temperature".to_string(), Value::from(0.3));  // æ›´ä¿å®ˆçš„å›ç­”
    options.insert("max_tokens".to_string(), Value::from(100));   // é™åˆ¶é•¿åº¦
    request.set_options(options);
    
    println!("å‘é€è‡ªå®šä¹‰é…ç½®è¯·æ±‚...");
    match client.chat(request).await {
        Ok(response) => {
            println!("âœ… è‡ªå®šä¹‰é…ç½®æˆåŠŸ");
            if let Some(message) = response.message {
                println!("å›å¤: {}", message.content);
            }
        }
        Err(e) => {
            println!("âŒ è¯·æ±‚å¤±è´¥: {}", e);
        }
    }
    
    Ok(())
}

/// å·¥å…·è°ƒç”¨ç¤ºä¾‹
async fn tool_example(model: &str) -> Result<()> {
    println!("\n=== å·¥å…·è°ƒç”¨ç¤ºä¾‹ ===");
    
    let client = OllamaClient::new("http://localhost:11434".to_string())?;
    
    // å®šä¹‰ä¸€ä¸ªè®¡ç®—å™¨å·¥å…·
    let calculator_tool = Tool {
        tool_type: "function".to_string(),
        function: ToolFunction {
            name: "calculate".to_string(),
            description: "æ‰§è¡Œæ•°å­¦è®¡ç®—".to_string(),
            parameters: serde_json::json!({
                "type": "object",
                "properties": {
                    "expression": {
                        "type": "string",
                        "description": "æ•°å­¦è¡¨è¾¾å¼ï¼Œå¦‚ï¼š2+3*4"
                    }
                },
                "required": ["expression"]
            }),
        },
    };
    
    let messages = vec![
        Message {
            role: "system".to_string(),
            content: "ä½ å¯ä»¥ä½¿ç”¨ calculate å·¥å…·æ¥è®¡ç®—æ•°å­¦è¡¨è¾¾å¼".to_string(),
            thinking: None,
            images: None,
            tool_calls: None,
            tool_name: None,
        },
        Message {
            role: "user".to_string(),
            content: "è¯·è®¡ç®— 15 + 27 * 3".to_string(),
            thinking: None,
            images: None,
            tool_calls: None,
            tool_name: None,
        },
    ];
    
    let request = OllamaChatRequest::new(model.to_string(), messages)
        .with_tools(vec![calculator_tool]);
    
    println!("å‘é€å·¥å…·è°ƒç”¨è¯·æ±‚...");
    match client.chat(request).await {
        Ok(response) => {
            println!("âœ… å·¥å…·è°ƒç”¨è¯·æ±‚æˆåŠŸ");
            if let Some(message) = response.message {
                println!("å›å¤: {}", message.content);
                
                if let Some(tool_calls) = &message.tool_calls {
                    println!("å·¥å…·è°ƒç”¨:");
                    for tool_call in tool_calls {
                        println!("  å‡½æ•°: {}", tool_call.function.name);
                        println!("  å‚æ•°: {:?}", tool_call.function.arguments);
                    }
                }
            }
        }
        Err(e) => {
            println!("âŒ å·¥å…·è°ƒç”¨å¤±è´¥: {}", e);
            println!("æ³¨æ„ï¼šéœ€è¦æ”¯æŒå·¥å…·è°ƒç”¨çš„æ¨¡å‹");
        }
    }
    
    Ok(())
}

#[tokio::main]
async fn main() -> Result<()> {
    println!("ğŸ¤– OllamaClient ä½¿ç”¨æ¼”ç¤º");
    println!("=======================");
    println!("è¯·ç¡®ä¿:");
    println!("1. Ollama æœåŠ¡æ­£åœ¨è¿è¡Œï¼šollama serve");
    println!("2. å·²å®‰è£…æ¨¡å‹ï¼šollama pull llama3.2");
    println!("3. æœåŠ¡åœ°å€ï¼šhttp://localhost:11434");
    println!("{}", "=".repeat(40));
    
    // åˆå§‹åŒ–å®¢æˆ·ç«¯å¹¶é€‰æ‹©æ¨¡å‹
    let client = OllamaClient::new("http://localhost:11434".to_string())?;
    let selected_model = match initialize_model(&client).await {
        Ok(model) => model,
        Err(e) => {
            println!("âŒ åˆå§‹åŒ–å¤±è´¥: {}", e);
            return Ok(());
        }
    };
    
    println!("{}", "=".repeat(40));
    
    // è¿è¡Œå„ç§ç¤ºä¾‹ï¼Œä½¿ç”¨é€‰å®šçš„æ¨¡å‹
    basic_example(&selected_model).await?;
    streaming_example(&selected_model).await?;
    custom_config_example(&selected_model).await?;
    tool_example(&selected_model).await?;
    
    println!("\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼");
    Ok(())
}
